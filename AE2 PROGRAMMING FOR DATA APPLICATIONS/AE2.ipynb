{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6965e10",
   "metadata": {},
   "source": [
    "# Course Title: Programming for Data Applications\n",
    "# Course Code: LDSCI7234\n",
    "# Course Leader: Dr. Jiri Motejlek\n",
    "# student ID: 23220031\n",
    "# NB: See report for detail explnation of the project\n",
    "# Model did not include in files, please download files and run the code. \n",
    "# Thank you for your troubles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac2de3",
   "metadata": {},
   "source": [
    "### Task One: Data Collection (20 Marks)\n",
    "- Collect a large and diverse textual dataset suitable for training word embeddings.\n",
    "\n",
    "- Ensure that the dataset is preprocessed: remove special characters, lowercase all words, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea8568",
   "metadata": {},
   "source": [
    "**The first objective is to detail the necessary steps for effectively cleaning text. These steps will be methodically applied to the text file titled \"Crime and Punishment\". Following this, I will develop a function specifically designed for preparing the training process.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1228fb",
   "metadata": {},
   "source": [
    "#### Method\n",
    "Step 1: Collect Data and Open in Jupyter Notebook\n",
    "- \"Crime and Punishment\" by Fyodor Dostoyevsky. Download from Project Gutenberg using the following link: https://www.gutenberg.org/ebooks/2554.txt.utf-8\n",
    "- Download the plain text \"UTF-8\" version of \"Crime and Punishment\" by Fyodor Dostoyevsky to my local drive.\n",
    "- Use `read()` to open the txt file (week5)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "59a4e9d7",
   "metadata": {},
   "source": [
    "with open(\"./CAP.txt\", \"r\") as file: \n",
    "    CAP_raw = file.read()\n",
    "    \n",
    "print(CAP_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee60ff",
   "metadata": {},
   "source": [
    "Step 2: Data preparation for Training\n",
    "##### text cleaning \n",
    "-> case normalisation -> contraction expansion -> tokenisation -> removing punctuations -> removing stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99262f3a",
   "metadata": {},
   "source": [
    "import gensim\n",
    "import string\n",
    "import nltk\n",
    "import contractions\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a54ea671",
   "metadata": {},
   "source": [
    "def convert_utf(text):\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\").replace('\\u201C', \"`\").replace('\\u201D', \"`\").replace('\\u2013', '-').replace('\\u2014', '-')\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    return text.decode('ascii')\n",
    "\n",
    "#convert file(CAP_raw), using function convert_utf\n",
    "converted_CAP = convert_utf(CAP_raw)\n",
    "\n",
    "print(converted_CAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088eb399",
   "metadata": {},
   "source": [
    "##### case normalisation（lower case）"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b43fada",
   "metadata": {},
   "source": [
    "normalised_CAP = converted_CAP.lower()\n",
    "\n",
    "print(normalised_CAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f226a",
   "metadata": {},
   "source": [
    "##### Remove whitespaces and non printable characters\n",
    "- apply strip(), and then use join() to combine the words into one string.\n",
    "- use split(), strip(), join() (week5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1712be1",
   "metadata": {},
   "source": [
    "#splitting the text\n",
    "split_CAP = normalised_CAP.split()\n",
    "#stripping the whitespaces and non-printable characters\n",
    "strip_CAP = [word.strip() for word in split_CAP]\n",
    "#joinning the texts together\n",
    "new_CAP = ' '.join(strip_CAP)\n",
    "\n",
    "print(new_CAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7c2f2",
   "metadata": {},
   "source": [
    "##### Tokenization\n",
    "- I will use Gensim to train a model, using the cleaned_CAP text.\n",
    "- Gensim expects the data to be in the format of [[word, word, word], [word, word, word], [word, word, word]]. #Week11\n",
    "- Therefore, we use: `sent_tokenize` + `word_tokenize`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bb50886",
   "metadata": {},
   "source": [
    "sentences = nltk.sent_tokenize(new_CAP)\n",
    "\n",
    "data_CAP = []\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    data_CAP.append(words)\n",
    "\n",
    "print(data_CAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1287e5d",
   "metadata": {},
   "source": [
    "##### removing punctuations\n",
    "- use string.punctuation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab2da089",
   "metadata": {},
   "source": [
    "import string\n",
    "\n",
    "cleaned_data_CAP = []\n",
    "\n",
    "for sublist in data_CAP:\n",
    "   \n",
    "    cleaned_sublist = [word for word in sublist if word not in string.punctuation]\n",
    "    cleaned_data_CAP.append(cleaned_sublist)\n",
    "\n",
    "print(cleaned_data_CAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e26722",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Remove stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9938ae56",
   "metadata": {},
   "source": [
    "import nltk\n",
    "\n",
    "#the processed_scropts contains a list of tokens (as I tokenised it before)\n",
    "tokens = cleaned_data_CAP\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "filtered_text = []\n",
    "for token in tokens:\n",
    "    if token.lower() not in stop_words:\n",
    "        filtered_text.append(token)\n",
    "\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0518c",
   "metadata": {},
   "source": [
    "##### word expansion #ref1: https://www.geeksforgeeks.org/nlp-expand-contractions-in-text-processing/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eeda0ad",
   "metadata": {},
   "source": [
    "import contractions\n",
    "#Creating an empty list\n",
    "expanded_words = [] \n",
    "\n",
    "#Splitting 'filtered_text' into words and expanding each word\n",
    "for word in filtered_text.split():\n",
    "    #Using contractions.fix to expand the word\n",
    "    expanded_words.append(contractions.fix(word)) \n",
    "\n",
    "#Joining the expanded words back into a string\n",
    "expanded_text = ' '.join(expanded_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8cf1c",
   "metadata": {},
   "source": [
    "#### Remove special charaters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43f06fcd",
   "metadata": {},
   "source": [
    "final_data_CAP = []\n",
    "\n",
    "for sublist in expanded_text:\n",
    "    filtered_sublist = [word for word in sublist if word not in ['---', '...']]\n",
    "    final_data_CAP.append(filtered_sublist)\n",
    "\n",
    "print(final_data_CAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25096e4b",
   "metadata": {},
   "source": [
    "##### Import all the modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdca7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import nltk\n",
    "import string\n",
    "import contractions\n",
    "import gensim\n",
    "from flask import Flask, request\n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c8b16",
   "metadata": {},
   "source": [
    "### Combine the above steps into a function to call to clean text files in preparation for training.\n",
    "\n",
    "#### Method:\n",
    "1. `convert_utf` function #week11\n",
    "   - Converts text files to Unicode UTF-8 to unify text files and potentially fix irrational text, question marks, and mojibake.\n",
    "2. `.lower()` #week5 \n",
    "   - Normalization to lowercase.\n",
    "3. `nltk.word_tokenize()` & `nltk.sent_tokenize()` #week9 & week11\n",
    "   - Tokenizing the text into sentences and then tokenizing each sentence into words, in the format of [[word, word, word], [word, word, word], [word, word, word]], and continuing to clean the text.\n",
    "4. `string.punctuation`\n",
    "   - Removing punctuation.\n",
    "5. `contractions.fix()`\n",
    "   - Expanding contracted words.\n",
    "6. `word.isdigit()`\n",
    "   - Removing numbers, as an examination of text from Project Gutenberg revealed numbers at the beginning of the text unrelated to the context of the book.\n",
    "7. Remove '---' and '...' using an if/not statement\n",
    "   - As I examined the text, I found these two symbols remaining.\n",
    "8. Remove stopwords.\n",
    "9. Add the cleaned words of each sentence to `training.data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f973ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_convert_text(raw_text):\n",
    "    #Define the function for converting UTF text format \n",
    "    def convert_utf(text): \n",
    "        text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\").replace('\\u201C', \"`\").replace('\\u201D', \"`\").replace('\\u2013', '-').replace('\\u2014', '-')\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = text.encode('ascii', 'ignore')\n",
    "        return text.decode('ascii')\n",
    "\n",
    "    #Step1: Convert to UTF characters\n",
    "    UTF_text = convert_utf(raw_text)\n",
    "\n",
    "    #Step2: Normalisation to lowercase\n",
    "    normalised_text = UTF_text.lower()\n",
    "\n",
    "    #Step3: Tokenisation\n",
    "    sentences = nltk.sent_tokenize(normalised_text)\n",
    "    training_data = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        #Step4: Remove punctuations\n",
    "        words_no_punctuations = [word for word in words if word not in string.punctuation]\n",
    "\n",
    "        #Step5: Expand contracted words\n",
    "        words_expanded = [contractions.fix(word) for word in words_no_punctuations]\n",
    "\n",
    "        #Step6: Remove numbers\n",
    "        words_no_numbers = [word for word in words_expanded if not word.isdigit()]\n",
    "\n",
    "        #Step7: Remove '---' and '...' \n",
    "        cleaned_words = [word for word in words_no_numbers if word not in ['---', '...', '***']]\n",
    "\n",
    "        #Step8: Remove stopwords\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        words_no_stopwords = [word for word in cleaned_words if word.lower() not in stop_words]\n",
    "\n",
    "        #Step9: create final training data\n",
    "        training_data.append(words_no_stopwords)\n",
    "        \n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7b7f7",
   "metadata": {},
   "source": [
    "### Task Two: Training [20marks]:\n",
    "-  Use a Word2Vec embeddings technique. \n",
    "- Utilise Gensim library to assist with the training.\n",
    "- Save the trained model for future use. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6bb570",
   "metadata": {},
   "source": [
    "##### USe Word2Vec\n",
    "- create an empty model#week11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(vector_size=230, min_count=3, sg=2)\n",
    "model.save(\"./model_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c498ae0",
   "metadata": {},
   "source": [
    "##### Training the Model #week11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./1CAP.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=False)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677933f1",
   "metadata": {},
   "source": [
    "##### Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e0c94",
   "metadata": {},
   "source": [
    "##### Train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5168232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./2WN.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be8920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c8377",
   "metadata": {},
   "source": [
    "##### Train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57593554",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./3BRS.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c880eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13080e78",
   "metadata": {},
   "source": [
    "##### Train4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./4GI.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b6bd2",
   "metadata": {},
   "source": [
    "##### Train5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c0b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./5NBU.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d280efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873ba97",
   "metadata": {},
   "source": [
    "##### Train6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./6TI.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e0f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211185d",
   "metadata": {},
   "source": [
    "##### Train7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4edff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./7TP.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68383061",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fb62d",
   "metadata": {},
   "source": [
    "##### Train8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./8ST.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1043b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ed339",
   "metadata": {},
   "source": [
    "##### Train9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./9BOK.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f74879",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0376a602",
   "metadata": {},
   "source": [
    "##### Train10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefdb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./10TG.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c6241",
   "metadata": {},
   "source": [
    "##### Train11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./11PF.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dca9766",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778aaf36",
   "metadata": {},
   "source": [
    "##### Train12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9be4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./12PLIS.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69869f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7bb60c",
   "metadata": {},
   "source": [
    "##### Train13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3254a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./13SC.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb02e87",
   "metadata": {},
   "source": [
    "##### Train14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb49d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./14UD.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b78ecf",
   "metadata": {},
   "source": [
    "#### After Adjusting the Parameters, Train Multiple Files Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9878a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"./1CD.txt\",\"./2CD.txt\",\"./3CD.txt\",\"./4CD.txt\",\"./5CD.txt\",\"./6CD.txt\",\"./7CD.txt\",\"./8CD.txt\",\"./9CD.txt\",\"./10CD.txt\", \"./11CD.txt\", \"./1VW.txt\", \"./2VW.txt\", \"./3VW.txt\", \"./4VW.txt\", \"./5VW.txt\", \"./6VW.txt\", \"./7VW.txt\", \"./8VW.txt\", \"./9VW.txt\", \"./10VW.txt\", \"./11VW.txt\", \"./1LT.txt\", \"./2LT.txt\", \"./3LT.txt\", \"./4LT.txt\", \"./5LT.txt\", \"./6LT.txt\", \"./7LT.txt\", \"./8LT.txt\", \"./9LT.txt\", \"./10LT.txt\", \"./11LT.txt\"]\n",
    "\n",
    "\n",
    "for file_name in file_names:\n",
    "    with open(file_name, \"r\", encoding=\"utf8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    data = clean_and_convert_text(content)\n",
    "\n",
    "    #train the model (set to True)\n",
    "    model.build_vocab(data, update=True)\n",
    "    model.train(data, total_examples=len(data), epochs=model.epochs)\n",
    "    \n",
    "    #Save the model\n",
    "    model.save('./model_test')\n",
    "    \n",
    "    print(\"Training Done On \" + file_name)\n",
    "\n",
    "print(\"Comeplete All Trainings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e81c5f",
   "metadata": {},
   "source": [
    "### Task Three: Web Application [20 marks]:\n",
    "- Design a simple web interface where a user can input a word. (10 marks)\n",
    "- Implement back-end functionality to fetch the opposite of the given word using the trained embeddings. (10 marks)\n",
    "- Return the opposite word to the user on the web interface.\n",
    "- Use the Flask library for the web application.\n",
    "\n",
    "##### Method \n",
    "- Alter code from week 12 to create the web interface.\n",
    "- Utilize the Flask framework.\n",
    "- Use the Word2Vec model's word embeddings formula: word1 - word2 + word3 = word4. ref2:https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01bf0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load my model\n",
    "model = gensim.models.Word2Vec.load(\"./model_test\")\n",
    "\n",
    "app = Flask(__name__)\n",
    "#create web interface\n",
    "html_form_with_message = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Find the Opposite Words!</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h2>Enter Your Word Please:)</h2>\n",
    "    <form method=\"post\" action=\"/\">\n",
    "        <label for=\"text\">Word:</label><br>\n",
    "        <input type=\"text\" name=\"input_word\"><br><br>\n",
    "        <input type=\"submit\" value=\"NOW FIND THE OPPOSITE\">\n",
    "    </form>\n",
    "    <p>Opposite Word: put_data_here</p>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def home():\n",
    "    user_input = ''\n",
    "    opposite_word = \"Not found\"\n",
    "    if request.method == 'POST':\n",
    "        user_input = request.form['input_word']\n",
    "        try:\n",
    "            \n",
    "            reference_pair = (\"wealthy\", \"poverty\")  \n",
    "\n",
    "            opposite_result = model.wv.most_similar(positive=[user_input, reference_pair[1]], negative=[reference_pair[0]], topn=1)\n",
    "            opposite_word = opposite_result[0][0] if opposite_result else \"Not found\"\n",
    "        except KeyError:\n",
    "            opposite_word = \"Word not in database\"\n",
    "\n",
    "    display_text = f\"Input word '{user_input}' - Opposite word: {opposite_word}\"\n",
    "    return html_form_with_message.replace(\"put_data_here\", display_text)\n",
    "\n",
    "app.run() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
