{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401fa8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course Title: Programming for Data Applications\n",
    "# Course Code: LDSCI7234\n",
    "# Course Leader: Dr. Jiri Motejlek\n",
    "# student ID: 23220031\n",
    "# NB: Please also download text file, alongside the submission, thank you:) \n",
    "## file name: \n",
    "## file (relative)path: \n",
    "### Additionally, the link to download: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac2de3",
   "metadata": {},
   "source": [
    "### Task One: Data Collection (20 Marks)\n",
    "- Collect a large and diverse textual dataset suitable for training word embeddings.\n",
    "\n",
    "- Ensure that the dataset is preprocessed: remove special characters, lowercase all words, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1228fb",
   "metadata": {},
   "source": [
    "#### Method\n",
    "Step 1: Collect data and open in jupyter notebook\n",
    "- \"Crime and Punishment\" by Fyodor Dostoyevsky. From Project Gutenberg, weblink to download: https://www.gutenberg.org/ebooks/2554.txt.utf-8  \n",
    "- Download the plain text \"UTF-8\" version of \"Crime and Punishment\" by Fyodor Dostoyevsky to my local drive.\n",
    "- use read() to open the txt file (week5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59a4e9d7",
   "metadata": {},
   "source": [
    "with open(\"./CAP.txt\", \"r\") as file: \n",
    "    CAP_raw = file.read()\n",
    "    \n",
    "print(CAP_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee60ff",
   "metadata": {},
   "source": [
    "Step 2: Data preparation for Training\n",
    "##### text cleaning \n",
    "-> case normalisation -> contraction expansion -> tokenisation -> removing punctuations -> removing stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99262f3a",
   "metadata": {},
   "source": [
    "import gensim\n",
    "import string\n",
    "import nltk\n",
    "import contractions\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a54ea671",
   "metadata": {},
   "source": [
    "def convert_utf(text):\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\").replace('\\u201C', \"`\").replace('\\u201D', \"`\").replace('\\u2013', '-').replace('\\u2014', '-')\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    return text.decode('ascii')\n",
    "\n",
    "# Assuming your file contents are stored in a variable named 'file_contents'\n",
    "# Call the convert_utf function with this text\n",
    "converted_CAP = convert_utf(CAP_raw)\n",
    "\n",
    "# Now, 'converted_text' contains the processed text\n",
    "# You can print it, write it to a file, or use it as needed\n",
    "print(converted_CAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088eb399",
   "metadata": {},
   "source": [
    "##### case normalisation（lower case）"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b43fada",
   "metadata": {},
   "source": [
    "normalised_CAP = converted_CAP.lower()\n",
    "\n",
    "print(normalised_CAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f226a",
   "metadata": {},
   "source": [
    "##### Remove whitespaces and non printable characters\n",
    "- apply strip(), and then use join() to combine the words into one string.\n",
    "- use split(), strip(), join() (week5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1712be1",
   "metadata": {},
   "source": [
    "#splitting the text\n",
    "split_CAP = normalised_CAP.split()\n",
    "#stripping the whitespaces and non-printable characters\n",
    "strip_CAP = [word.strip() for word in split_CAP]\n",
    "#joinning the texts together\n",
    "new_CAP = ' '.join(strip_CAP)\n",
    "\n",
    "print(new_CAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7c2f2",
   "metadata": {},
   "source": [
    "##### Tokenisation\n",
    "- I will use Genism to train a model, using the cleaned_CAP text.\n",
    "- Genism expects the data to be in the format of [[word, word, word], [word, word, word], [word, word, word]]. #Week11\n",
    "- Therefore, we use: sent_tokenize + word_tokenize"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bb50886",
   "metadata": {},
   "source": [
    "sentences = nltk.sent_tokenize(new_CAP)\n",
    "\n",
    "data_CAP = []\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    data_CAP.append(words)\n",
    "\n",
    "print(data_CAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1287e5d",
   "metadata": {},
   "source": [
    "##### removing punctuations\n",
    "- use string.punctuation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab2da089",
   "metadata": {},
   "source": [
    "import string\n",
    "\n",
    "# Assuming data_CAP is a list of lists with tokenized words\n",
    "cleaned_data_CAP = []\n",
    "\n",
    "for sublist in data_CAP:\n",
    "    # Remove punctuation from each word in the sublist\n",
    "    cleaned_sublist = [word for word in sublist if word not in string.punctuation]\n",
    "    cleaned_data_CAP.append(cleaned_sublist)\n",
    "\n",
    "# Printing the entire list to view the result\n",
    "print(cleaned_data_CAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e26722",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Remove stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9938ae56",
   "metadata": {},
   "source": [
    "import nltk\n",
    "\n",
    "#the processed_scropts contains a list of tokens (as I tokenised it before)\n",
    "tokens = cleaned_data_CAP\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "filtered_text = []\n",
    "for token in tokens:\n",
    "    if token.lower() not in stop_words:\n",
    "        filtered_text.append(token)\n",
    "\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0518c",
   "metadata": {},
   "source": [
    "##### word expansion #ref1: https://www.geeksforgeeks.org/nlp-expand-contractions-in-text-processing/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eeda0ad",
   "metadata": {},
   "source": [
    "import contractions\n",
    "#Creating an empty list\n",
    "expanded_words = [] \n",
    "\n",
    "#Splitting 'filtered_text' into words and expanding each word\n",
    "for word in filtered_text.split():\n",
    "    #Using contractions.fix to expand the word\n",
    "    expanded_words.append(contractions.fix(word)) \n",
    "\n",
    "#Joining the expanded words back into a string\n",
    "expanded_text = ' '.join(expanded_words)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43f06fcd",
   "metadata": {},
   "source": [
    "#Assuming cleaned_data_CAP is your cleaned data in a list of lists format\n",
    "final_data_CAP = []\n",
    "\n",
    "for sublist in expanded_text:\n",
    "    filtered_sublist = [word for word in sublist if word not in ['---', '...']]\n",
    "    final_data_CAP.append(filtered_sublist)\n",
    "\n",
    "# View the result\n",
    "print(final_data_CAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25096e4b",
   "metadata": {},
   "source": [
    "##### Import all the modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfdca7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import nltk\n",
    "import string\n",
    "import contractions\n",
    "import gensim\n",
    "from flask import Flask, request\n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c8b16",
   "metadata": {},
   "source": [
    "### Combine the above steps into a function to call to clean txt files for preparation of training.\n",
    "\n",
    "#### Method:\n",
    "1. convert_utf function #week11\n",
    "- converts text files to unicode UTF-8 can unified text files and potentially fixed irrational text, question marks, and mojibake.\n",
    "2. .lower() #week5 \n",
    "- normalisation to lowercase\n",
    "3. nltk.word_tokenize()& nltk.sent_tokenize() #week9 & week11\n",
    "- tokenizing the text into sentences and then tokenizing each sentence into words, [[word, word, word], [word, word, word], [word, word, word]]. and within continue cleaning text.\n",
    "4. string.punctuation\n",
    "- remove punctuations\n",
    "5. contractions.fix()\n",
    "- expand contracted words\n",
    "6. word.isdigit()\n",
    "- as i examined text from project gutenberg, I realised there are numbers in the beginning of the text that is unrelated to the context of the book.\n",
    "7. remove '---' '...' using if/not statement\n",
    "- as I exmaied the text I found these two symbols remains\n",
    "8. remove stopwords\n",
    "9. add the cleanned words of sentence to training.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f973ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_convert_text(raw_text):\n",
    "    #Define the function for converting UTF text format \n",
    "    def convert_utf(text): \n",
    "        text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\").replace('\\u201C', \"`\").replace('\\u201D', \"`\").replace('\\u2013', '-').replace('\\u2014', '-')\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = text.encode('ascii', 'ignore')\n",
    "        return text.decode('ascii')\n",
    "\n",
    "    #Step1: Convert to UTF characters\n",
    "    UTF_text = convert_utf(raw_text)\n",
    "\n",
    "    #Step2: Normalisation to lowercase\n",
    "    normalised_text = UTF_text.lower()\n",
    "\n",
    "    #Step3: Tokenisation\n",
    "    sentences = nltk.sent_tokenize(normalised_text)\n",
    "    training_data = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        #Step4: Remove punctuations\n",
    "        words_no_punctuations = [word for word in words if word not in string.punctuation]\n",
    "\n",
    "        #Step5: Expand contracted words\n",
    "        words_expanded = [contractions.fix(word) for word in words_no_punctuations]\n",
    "\n",
    "        #Step6: Remove numbers\n",
    "        words_no_numbers = [word for word in words_expanded if not word.isdigit()]\n",
    "\n",
    "        #Step7: Remove '---' and '...' \n",
    "        cleaned_words = [word for word in words_no_numbers if word not in ['---', '...', '***']]\n",
    "\n",
    "        #Step8: Remove stopwords\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        words_no_stopwords = [word for word in cleaned_words if word.lower() not in stop_words]\n",
    "\n",
    "        #Step9: create final training data\n",
    "        training_data.append(words_no_stopwords)\n",
    "        \n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7b7f7",
   "metadata": {},
   "source": [
    "### Task Two: Training [20marks]:\n",
    "-  Use a Word2Vec embeddings technique. \n",
    "- Utilise Gensim library to assist with the training.\n",
    "- Save the trained model for future use. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6bb570",
   "metadata": {},
   "source": [
    "##### USe Word2Vec\n",
    "- create an empty model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "841c9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(vector_size=230, min_count=3, sg=2)\n",
    "model.save(\"./model_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c498ae0",
   "metadata": {},
   "source": [
    "##### Training the Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c21d087",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './1CAP.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./1CAP.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:    \n\u001b[1;32m      2\u001b[0m     content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m clean_and_convert_text(content)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './1CAP.txt'"
     ]
    }
   ],
   "source": [
    "with open(\"./1CAP.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=False)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677933f1",
   "metadata": {},
   "source": [
    "##### Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e0c94",
   "metadata": {},
   "source": [
    "##### Train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5168232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./2WN.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be8920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c8377",
   "metadata": {},
   "source": [
    "##### Train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57593554",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./3BRS.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c880eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13080e78",
   "metadata": {},
   "source": [
    "##### Train4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./4GI.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b6bd2",
   "metadata": {},
   "source": [
    "##### Train5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c0b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./5NBU.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d280efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873ba97",
   "metadata": {},
   "source": [
    "##### Train6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./6TI.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e0f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211185d",
   "metadata": {},
   "source": [
    "##### Train7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4edff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./7TP.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68383061",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fb62d",
   "metadata": {},
   "source": [
    "##### Train8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./8ST.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1043b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ed339",
   "metadata": {},
   "source": [
    "##### Train9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./9BOK.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f74879",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0376a602",
   "metadata": {},
   "source": [
    "##### Train10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefdb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./10TG.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c6241",
   "metadata": {},
   "source": [
    "##### Train11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./11PF.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dca9766",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778aaf36",
   "metadata": {},
   "source": [
    "##### Train12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9be4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./12PLIS.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69869f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7bb60c",
   "metadata": {},
   "source": [
    "##### Train13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3254a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./13SC.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb02e87",
   "metadata": {},
   "source": [
    "##### Train14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb49d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./14UD.txt\", \"r\", encoding=\"utf8\") as file:    \n",
    "    content = file.read()\n",
    "\n",
    "data = clean_and_convert_text(content)\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('sad', topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9878a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"./1CD.txt\",\"./2CD.txt\",\"./3CD.txt\",\"./4CD.txt\",\"./5CD.txt\",\"./6CD.txt\",\"./7CD.txt\",\"./8CD.txt\",\"./9CD.txt\",\"./10CD.txt\", \"./11CD.txt\"]\n",
    "\n",
    "\n",
    "for file_name in file_names:\n",
    "    with open(file_name, \"r\", encoding=\"utf8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    data = clean_and_convert_text(content)\n",
    "\n",
    "    # Build and train the Word2Vec model\n",
    "    model.build_vocab(data, update=True)\n",
    "    model.train(data, total_examples=len(data), epochs=model.epochs)\n",
    "    \n",
    "    # Save the model after each file processing\n",
    "    model.save('./model_test')\n",
    "    \n",
    "    print(\"Finished training on \" + file_name)\n",
    "\n",
    "# Model training is complete after processing all files\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e81c5f",
   "metadata": {},
   "source": [
    "### Task Three: WebApplication[20marks]:\n",
    "- Design a simple web interface where a user can input a word. (10 marks)\n",
    "- Implement back-end functionality to fetch the opposite of the given word, using the trained embeddings. (10 marks)\n",
    "- Return the opposite word to the user on the web interface.\n",
    "- Use Flask library for the web application.\n",
    "\n",
    "##### method \n",
    "- alter code from week 12 to create the web interface \n",
    "- using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01bf0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load my model\n",
    "model = gensim.models.Word2Vec.load(\"./model_test\")\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "html_form_with_message = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Find the Opposite Words</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h2>Enter Your Word Please:)</h2>\n",
    "    <form method=\"post\" action=\"/\">\n",
    "        <label for=\"text\">Word:</label><br>\n",
    "        <input type=\"text\" name=\"input_word\"><br><br>\n",
    "        <input type=\"submit\" value=\"NOW FIND THE OPPOSITE\">\n",
    "    </form>\n",
    "    <p>Opposite Word: put_data_here</p>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def home():\n",
    "    user_input = ''\n",
    "    opposite_word = \"Not found\"\n",
    "    if request.method == 'POST':\n",
    "        user_input = request.form['input_word']\n",
    "        try:\n",
    "            \n",
    "            reference_pair = (\"wealthy\", \"poverty\")  \n",
    "\n",
    "            opposite_result = model.wv.most_similar(positive=[user_input, reference_pair[1]], negative=[reference_pair[0]], topn=1)\n",
    "            opposite_word = opposite_result[0][0] if opposite_result else \"Not found\"\n",
    "        except KeyError:\n",
    "            opposite_word = \"Word not in database\"\n",
    "\n",
    "    display_text = f\"Input word '{user_input}' - Opposite word: {opposite_word}\"\n",
    "    return html_form_with_message.replace(\"put_data_here\", display_text)\n",
    "\n",
    "app.run() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
